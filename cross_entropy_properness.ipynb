{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "797b7c3f",
   "metadata": {},
   "source": [
    "The properness of cross entropy\n",
    "===========================\n",
    "\n",
    "Definition of a proper loss function from from paragraph 1 of Section 3 in page 3 Cid-Sueiro 2012.\n",
    "\n",
    "A loss function $l(\\mathbf{y}, \\mathbf{q})$ is proper, if the true probability vector $\\mathbf{p}$ is a member of $\\arg \\min_\\mathbf{q} \\mathbb{E}[l(\\mathbf{y}, \\mathbf{q})]$. And it is strictly proper if $\\mathbf{q}$ is the only minimizer of the set.\n",
    "\n",
    "Cid-Sueiro, Jesús. 2012. “Proper Losses for Learning from Partial Labels.” Pp. 1574–82 in Advances in Neural Information Processing Systems 25.\n",
    "\n",
    "Example 1: True probability $\\mathbf{p} = [0.8, 0.2, 0.0]$\n",
    "---------------------------------------------------\n",
    "\n",
    "Imagine that our model always predicts the probability $\\mathbf{q} = [0.8, 0.2, 0.0]$.\n",
    "\n",
    "We can sample once from the entire population and obtain one sample of the form $\\mathbf{y}=[1, 0, 0]$, if we compute the CE on this sample we would obtain\n",
    "\n",
    "$$CE([1, 0, 0], [0.8, 0.2, 0.0]) = -1\\log(0.8) - 0\\log(0.2) - 0\\log(0.0) = -\\log(0.8)$$\n",
    "\n",
    "However, if we keep sampling from the true distribution $\\mathbf{p} = [0.8, 0.2, 0.0]$, 20\\% of the times we would also obtain a label of the form $\\mathbf{y}=[0, 1, 0]$\n",
    "\n",
    "$$CE([0, 1, 0], [0.8, 0.2, 0.0]) = -0\\log(0.8) - 1\\log(0.2) - 0\\log(0.0) = -\\log(0.2)$$\n",
    "\n",
    "The definition of a proper loss does not have into account one individual sample, but the minimization of the loss as an expectation over the full population. Then, in the previous case and the entire population we should expect\n",
    "\n",
    "- that 80\\% of the times $CE([1, 0, 0], [0.8, 0.2, 0.0]) = -\\log(0.8)$\n",
    "- that 20\\% of the times $CE([0, 1, 0], [0.8, 0.2, 0.0]) = -\\log(0.2)$\n",
    "- that 0\\% of the times  $CE([0, 0, 1], [0.8, 0.2, 0.0]) = -\\log(0.0)$\n",
    "\n",
    "This corresponds to the following expectation\n",
    "\n",
    "$$\\mathbb{E}[CE([0.8, 0.2, 0.0], [0.8, 0.2, 0.0]) = -(0.8\\log(0.8) + 0.2\\log(0.2))$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9ece7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected CE = 0.5004024235381879\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def expected_ce(p, q):\n",
    "    #q = np.clip(p, np.finfo(float).eps, 1)\n",
    "    #q /= q.sum()\n",
    "    non_zero_p = p != 0\n",
    "    return -p[non_zero_p] @ np.log(q[non_zero_p])\n",
    "\n",
    "p = np.array([0.8, 0.2, 0.0])\n",
    "q = np.array([0.8, 0.2, 0.0])\n",
    "\n",
    "print('Expected CE = {}'.format(expected_ce(p, q)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfebe188",
   "metadata": {},
   "source": [
    "On the contrary, if the model predicts the probability $\\mathbf{q} = [0.8, 0.1, 0.1]$ we should expect\n",
    "\n",
    "- that 80\\% of the times the CE would be $-\\log(0.8)$\n",
    "- that 20\\% of the times CE = $-\\log(0.1)$\n",
    "- that 0\\% of the times CE = $-\\log(0.1)$\n",
    "\n",
    "This corresponds to the following expectation\n",
    "\n",
    "$$\\mathbb{E}[CE(\\mathbf{y}, [0.8, 0.1, 0.1]) = -(0.8\\log(0.8) + 0.2\\log(0.1))$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0d1dde1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected CE = 0.639031859650177\n"
     ]
    }
   ],
   "source": [
    "p = np.array([0.8, 0.2, 0.0])\n",
    "q = np.array([0.8, 0.1, 0.1])\n",
    "\n",
    "print('Expected CE = {}'.format(expected_ce(p, q)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb792e67",
   "metadata": {},
   "source": [
    "Example 2: True probability $\\mathbf{p} = [0.8, 0.1, 0.1]$\n",
    "---------------------------------------------------\n",
    "\n",
    "If the true probability distribution is $\\mathbf{p} = [0.8, 0.2, 0.0]$ and our model predicts $\\mathbf{q} = [0.8, 0.1, 0.1]$ we should expect\n",
    "\n",
    "- that 80\\% of the times the CE would be $-\\log(0.8)$\n",
    "- that 10\\% of the times CE = $-\\log(0.2)$\n",
    "- that 10\\% of the times CE = $-\\log(0)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8adc402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected CE = 0.639031859650177\n"
     ]
    }
   ],
   "source": [
    "p = np.array([0.8, 0.1, 0.1])\n",
    "q = np.array([0.8, 0.1, 0.1])\n",
    "\n",
    "print('Expected CE = {}'.format(expected_ce(p, q)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c0f1596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected CE = inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-a31765648b70>:7: RuntimeWarning: divide by zero encountered in log\n",
      "  return -p[non_zero_p] @ np.log(q[non_zero_p])\n"
     ]
    }
   ],
   "source": [
    "p = np.array([0.8, 0.1, 0.1])\n",
    "q = np.array([0.8, 0.2, 0.0])\n",
    "\n",
    "print('Expected CE = {}'.format(expected_ce(p, q)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5fa45f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
